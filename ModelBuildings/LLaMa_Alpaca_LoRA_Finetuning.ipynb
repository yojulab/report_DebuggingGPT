{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yojulab/report_DebuggingGPT/blob/main/ModelBuildings/LLaMa_Alpaca_LoRA_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlTEMQciQS9W"
      },
      "source": [
        "# LLaMa-Alpaca-LoRA 파인튜닝\n",
        "\n",
        "### refer\n",
        "##### youtube : https://youtu.be/aUXwVp4eUH4\n",
        "##### colab : https://colab.research.google.com/drive/1B0kaiAPHt1n9SKDSmUE6asFmQVxfC3JS?usp=sharing#scrollTo=5Dw8-wbgIrJ0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6Cm99ykUOUF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7dd0f4-20fb-4a2e-b09c-01c8f17c9d28"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TbpHMW2dIck_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "995b2785-7d1e-4501-b3ce-9cd481c973a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May 22 11:56:30 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLkPRXXMPInO"
      },
      "source": [
        "## Download source code and install dependencies\n",
        "\n",
        "- https://github.com/tloen/alpaca-lora.git\n",
        "- 주의!: bitsandbytes가 현재 Linux에서만 동작함\n",
        "- 주의!!: peft 버그 https://github.com/tloen/alpaca-lora/issues/293"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XpbGPksUG4Uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab90bef-cc46-4bf0-853f-92c0015762a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/alpaca-lora\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Found existing installation: peft 0.4.0.dev0\n",
            "Uninstalling peft-0.4.0.dev0:\n",
            "  Successfully uninstalled peft-0.4.0.dev0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!git clone -q https://github.com/kairess/alpaca-lora.git\n",
        "%cd alpaca-lora\n",
        "!pip install -r requirements.txt -q\n",
        "#!pip uninstall transformers -y\n",
        "#!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip uninstall peft -y\n",
        "!pip install -q git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZfGYCcyElu3"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "### Debugging 일지로 수집된 Instruction, Input, Response set.\n",
        "\n",
        "- https://github.com/yojulab/report_DebuggingGPT/blob/main/Datas/self_instructs_output.json\n",
        "\n",
        "\n",
        "```json\n",
        "[\n",
        "   {\n",
        "        \"instruction\": \"print(first)\",\n",
        "        \"input\": \"----> 1 print(first)\\n\\nNameError: name 'first' is not defined\",\n",
        "        \"output\": \"first = 'KIM'\\nprint(first)\\n\"\n",
        "    },\n",
        "    {\n",
        "        \"instruction\": \"Empty1 = \\\" \\\", Empty2 = \\\" \\\", Empty3 = \\\" \\\"\",\n",
        "        \"input\": \" Empty1 = \\\" \\\"  , Empty2 = \\\" \\\" , Empty3 = \\\" \\\"\\n    ^\\nSyntaxError: invalid syntax. Maybe you meant '==' or ':=' instead of '='?\",\n",
        "        \"output\": \"Empty1 = \\\" \\\" \\nEmpty2 = \\\" \\\" \\nEmpty3 = \\\" \\\"\"\n",
        "    },\n",
        "     ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yojulab/report_DebuggingGPT/main/Datas/self_instructs_output.json -O self_instructs_output.json"
      ],
      "metadata": {
        "id": "nE40boqUWCdD",
        "outputId": "87038e17-b4b8-4929-b07a-9e21025ed68e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-22 13:56:00--  https://raw.githubusercontent.com/yojulab/report_DebuggingGPT/main/Datas/self_instructs_output.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12148 (12K) [text/plain]\n",
            "Saving to: ‘self_instructs_output.json’\n",
            "\n",
            "\r          self_inst   0%[                    ]       0  --.-KB/s               \rself_instructs_outp 100%[===================>]  11.86K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-22 13:56:00 (123 MB/s) - ‘self_instructs_output.json’ saved [12148/12148]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "bd1Az-huVz9k",
        "outputId": "e4a38b26-b6b7-4d8d-b0bf-9ad1f1e8a75b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpaca_data_cleaned_archive.json  generate.py\n",
            "alpaca_data_gpt4.json             lengths.ipynb\n",
            "alpaca_data.json                  LICENSE\n",
            "data.json                         pyproject.toml\n",
            "DATA_LICENSE                      README.md\n",
            "docker-compose.yml                requirements.txt\n",
            "Dockerfile                        result.jpg\n",
            "export_hf_checkpoint.py           self_instructs_output.json\n",
            "export_state_dict_checkpoint.py   \u001b[0m\u001b[01;34mtemplates\u001b[0m/\n",
            "finetune.py                       train.sh\n",
            "\u001b[01;34mflagged\u001b[0m/                          \u001b[01;34mutils\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "am4EVblZIh4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0421b9c8-b02b-43c3-dc0c-55e82b4f93cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'instruction': 'E1 = \"\", E2 = \"\"',\n",
              "  'input': 'line 1\\n    E1 = \"\", E2 = \"\"\\n    ^\\nSyntaxError: invalid syntax. Maybe you meant \\'==\\' or \\':=\\' instead of \\'=\\'?',\n",
              "  'output': 'E1 = \"\"\\nE2 = \"\"'},\n",
              " {'instruction': 'Print(\"오상훈\")',\n",
              "  'input': '----> 1 Print(\"오상훈\")\\n\\nNameError: name \\'Print\\' is not defined',\n",
              "  'output': 'print(\"오상훈\")'},\n",
              " {'instruction': 'Wind = \"\", Wish = \"\"',\n",
              "  'input': 'line 1\\n    Wind = \"\", Wish = \"\"\\n    ^\\nSyntaxError: invalid syntax. Maybe you meant \\'==\\' or \\':=\\' instead of \\'=\\'?',\n",
              "  'output': 'Wind = \"\"\\nWish = \"\"'},\n",
              " {'instruction': 'Print(\"한희수\")',\n",
              "  'input': '----> 1 Print(\"한희수\")\\n\\nNameError: name \\'Print\\' is not defined',\n",
              "  'output': 'print(\"한희수\")'},\n",
              " {'instruction': 'if D = 9 :\\n  B = 5\\n  print(B)\\nelse :\\n  B = 200\\n  print(B)',\n",
              "  'input': \"line 2\\n    if D = 9 :\\n       ^\\nSyntaxError: invalid syntax. Maybe you meant '==' or ':=' instead of '='?\",\n",
              "  'output': 'if D == 9 :\\n  B = 5\\n  print(B)\\nelse :\\n  B = 200\\n  print(B)'}]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "with open('self_instructs_output.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "dataset[5:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1BQPeff_Coj"
      },
      "source": [
        "## Finetune\n",
        "\n",
        "기본 설정으로 했을 때:\n",
        "- 3h/1epoch in NVIDIA A100\n",
        "- 12h/1epoch in NVIDIA T4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwwJCbf9_yA"
      },
      "source": [
        "### Create a custom template\n",
        "\n",
        "Default Alpaca\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"description\": \"Template used by Alpaca-LoRA.\",\n",
        "    \"prompt_input\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n",
        "    \"prompt_no_input\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n",
        "    \"response_split\": \"### Response:\"    \n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "hL5c3kRs-Rcj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "prompt_template = {\n",
        "    \"description\": \"Alpaca-LoRA Custom 템플릿\",\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
        "        \"아래는 작업을 설명하는 명령어와 추가적 맥락을 제공하는 입력이 짝을 이루는 예제입니다.\\n\\n\"\n",
        "        \"Write a response that appropriately completes the request.\\n요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
        "        \"### Instruction(명령어):\\n{instruction}\\n\\n### Input(입력):\\n{input}\\n\\n### Response:\\n\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task.\\n\"\n",
        "        \"아래는 작업을 설명하는 명령어입니다.\\n\\n\"\n",
        "        \"Write a response that appropriately completes the request.\\n명령어에 따른 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n\"\n",
        "        \"### Instruction(명령어):\\n{instruction}\\n\\n### Response:\\n\"\n",
        "    ),\n",
        "    \"response_split\": \"### Response:\",\n",
        "}\n",
        "\n",
        "with open('templates/custom.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(prompt_template, f, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuOOeDPoTMSb"
      },
      "source": [
        "### Hyperparameters\n",
        "\n",
        "https://github.com/tloen/alpaca-lora/blob/0e1a5d52a460d14aea2325e43c302972badb9cdd/finetune.py#L28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 권장 학습 파라메터\n",
        "# !python finetune.py \\\n",
        "#     --base_model 'decapoda-research/llama-7b-hf' \\\n",
        "#     --data_path 'self_instructs_output.json' \\\n",
        "#     --output_dir './output' \\\n",
        "#     --num_epochs 3 \\\n",
        "#     --learning_rate 5e-5 \\\n",
        "#     --val_set_size 2000 \\\n",
        "#     --batch_size 512 \\\n",
        "#     --micro_batch_size 16 \\\n",
        "#     --prompt_template_name 'custom'"
      ],
      "metadata": {
        "id": "07FM__9NxFzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "5Dw8-wbgIrJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2968d0-658c-4df3-f3f2-81ff1c4c8deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-22 14:05:16.813852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-70jr6b3znmov --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Training Alpaca-LoRA model with params:\n",
            "base_model: decapoda-research/llama-7b-hf\n",
            "data_path: self_instructs_output.json\n",
            "output_dir: ./output\n",
            "batch_size: 512\n",
            "micro_batch_size: 16\n",
            "num_epochs: 1\n",
            "learning_rate: 0.0005\n",
            "cutoff_len: 256\n",
            "val_set_size: 10\n",
            "lora_r: 8\n",
            "lora_alpha: 16\n",
            "lora_dropout: 0.05\n",
            "lora_target_modules: ['q_proj', 'v_proj']\n",
            "train_on_inputs: True\n",
            "add_eos_token: False\n",
            "group_by_length: False\n",
            "wandb_project: \n",
            "wandb_run_name: \n",
            "wandb_watch: \n",
            "wandb_log_model: \n",
            "resume_from_checkpoint: False\n",
            "prompt template: custom\n",
            "\n",
            "Loading checkpoint shards: 100% 33/33 [01:16<00:00,  2.32s/it]\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-d2f0fefe39599590/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
            "100% 1/1 [00:00<00:00, 25.18it/s]\n",
            "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n",
            "  0% 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "{'train_runtime': 61.9714, 'train_samples_per_second': 0.678, 'train_steps_per_second': 0.016, 'train_loss': 0.1672748625278473, 'epoch': 1.0}\n",
            "100% 1/1 [01:01<00:00, 61.95s/it]\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/alpaca-lora/\u001b[0m\u001b[1;33mfinetune.py\u001b[0m:\u001b[94m283\u001b[0m in \u001b[92m<module>\u001b[0m                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m280 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m283 \u001b[2m│   \u001b[0mfire.Fire(train)                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m284 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m141\u001b[0m in \u001b[92mFire\u001b[0m             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   \u001b[0mcontext.update(caller_globals)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   \u001b[0mcontext.update(caller_locals)                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m141 \u001b[2m  \u001b[0mcomponent_trace = _Fire(component, args, parsed_flag_args, context,  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mif\u001b[0m component_trace.HasError():                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   \u001b[0m_DisplayError(component_trace)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m475\u001b[0m in \u001b[92m_Fire\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m472 \u001b[0m\u001b[2m│     \u001b[0mis_class = inspect.isclass(component)                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m473 \u001b[0m\u001b[2m│     \u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m474 \u001b[0m\u001b[2m│     \u001b[0m\u001b[94mtry\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m475 \u001b[2m│   │   \u001b[0mcomponent, remaining_args = _CallAndUpdateTrace(               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m476 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomponent,                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m477 \u001b[0m\u001b[2m│   │   │   \u001b[0mremaining_args,                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m478 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomponent_trace,                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m691\u001b[0m in                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_CallAndUpdateTrace\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m688 \u001b[0m\u001b[2m│   \u001b[0mloop = asyncio.get_event_loop()                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m689 \u001b[0m\u001b[2m│   \u001b[0mcomponent = loop.run_until_complete(fn(*varargs, **kwargs))        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m690 \u001b[0m\u001b[2m  \u001b[0m\u001b[94melse\u001b[0m:                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m691 \u001b[2m│   \u001b[0mcomponent = fn(*varargs, **kwargs)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m692 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m693 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mif\u001b[0m treatment == \u001b[33m'\u001b[0m\u001b[33mclass\u001b[0m\u001b[33m'\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m694 \u001b[0m\u001b[2m│   \u001b[0maction = trace.INSTANTIATED_CLASS                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/alpaca-lora/\u001b[0m\u001b[1;33mfinetune.py\u001b[0m:\u001b[94m275\u001b[0m in \u001b[92mtrain\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m272 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m273 \u001b[0m\u001b[2m│   \u001b[0mtrainer.train(resume_from_checkpoint=resume_from_checkpoint)       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m274 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m275 \u001b[2m│   \u001b[0mmodel.save_pretrained(output_dir)                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m276 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m277 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m278 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m If there\u001b[0m\u001b[33m'\u001b[0m\u001b[33ms a warning about missing keys above, please disr\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m125\u001b[0m in            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92msave_pretrained\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 122 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 123 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m adapter_name, peft_config \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.peft_config.items():    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 124 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# save only the trainable weights\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 125 \u001b[2m│   │   │   \u001b[0moutput_state_dict = get_peft_model_state_dict(            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 126 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m, state_dict=kwargs.get(\u001b[33m\"\u001b[0m\u001b[33mstate_dict\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m), adap \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 127 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 128 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput_dir = os.path.join(save_directory, adapter_name) \u001b[94mi\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/peft/utils/\u001b[0m\u001b[1;33msave_and_load.py\u001b[0m:\u001b[94m32\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mget_peft_model_state_dict\u001b[0m                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   \u001b[0mconfig = model.peft_config[adapter_name]                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 31 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m state_dict \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 32 \u001b[2m│   │   \u001b[0mstate_dict = model.state_dict()                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m config.peft_type \u001b[95min\u001b[0m (PeftType.LORA, PeftType.ADALORA):          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# to_return = lora_state_dict(model, bias=model.peft_config.bi\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 35 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# adapted from `https://github.com/microsoft/LoRA/blob/main/lo\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/alpaca-lora/\u001b[0m\u001b[1;33mfinetune.py\u001b[0m:\u001b[94m266\u001b[0m in \u001b[92m<lambda>\u001b[0m                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m263 \u001b[0m\u001b[2m│   \u001b[0mold_state_dict = model.state_dict                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m\u001b[2m│   \u001b[0mmodel.state_dict = (                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m265 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mlambda\u001b[0m \u001b[96mself\u001b[0m, *_, **__: get_peft_model_state_dict(              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m266 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, old_state_dict()                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m267 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m268 \u001b[0m\u001b[2m│   \u001b[0m).\u001b[92m__get__\u001b[0m(model, \u001b[96mtype\u001b[0m(model))                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m269 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1818\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1818 \u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._state_dict_hooks.values():                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, destination, prefix, local_metad \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m hook_result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1818\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1818 \u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._state_dict_hooks.values():                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, destination, prefix, local_metad \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m hook_result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1818\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1818 \u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._state_dict_hooks.values():                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, destination, prefix, local_metad \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m hook_result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1818\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1818 \u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._state_dict_hooks.values():                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, destination, prefix, local_metad \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m hook_result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1818\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1818 \u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._state_dict_hooks.values():                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, destination, prefix, local_metad \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m hook_result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1818\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1818 \u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._state_dict_hooks.values():                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, destination, prefix, local_metad \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m hook_result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1818\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1815 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1818 \u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1819 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m hook \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._state_dict_hooks.values():                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1820 \u001b[0m\u001b[2m│   │   │   \u001b[0mhook_result = hook(\u001b[96mself\u001b[0m, destination, prefix, local_metad \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1821 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m hook_result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1815\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mstate_dict\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1812 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mhasattr\u001b[0m(destination, \u001b[33m\"\u001b[0m\u001b[33m_metadata\u001b[0m\u001b[33m\"\u001b[0m):                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1813 \u001b[0m\u001b[2m│   │   │   \u001b[0mdestination._metadata[prefix[:-\u001b[94m1\u001b[0m]] = local_metadata       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1814 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1815 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._save_to_state_dict(destination, prefix, keep_vars)      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1816 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m name, module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m._modules.items():                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m module \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1818 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mmodule.state_dict(destination=destination, prefix=pre \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/\u001b[0m\u001b[1;33mmodules.py\u001b[0m:\u001b[94m268\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_save_to_state_dict\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m265 \u001b[0m\u001b[2m│   │   \u001b[0m                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m266 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m267 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m reorder_layout:                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m268 \u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.weight.data = undo_layout(\u001b[96mself\u001b[0m.state.CxB, \u001b[96mself\u001b[0m.st \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m269 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m270 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96msuper\u001b[0m()._save_to_state_dict(destination, prefix, keep_vars \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m271 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/\u001b[0m\u001b[1;33m_functions.py\u001b[0m: \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[94m100\u001b[0m in \u001b[92mundo_layout\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m│   \u001b[0moutputs[tile_indices.flatten()] = tensor                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 98 \u001b[0m\u001b[2m│   \u001b[0moutputs = outputs.reshape(tile_rows, tile_cols, cols // tile_cols, \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   \u001b[0moutputs = outputs.permute(\u001b[94m3\u001b[0m, \u001b[94m0\u001b[0m, \u001b[94m2\u001b[0m, \u001b[94m1\u001b[0m)  \u001b[2m# (rows // tile_rows, tile_\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m100 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m outputs.reshape(rows, cols).contiguous()                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m102 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m103 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mMatMul8bit\u001b[0m(torch.autograd.Function):                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m44.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.75\u001b[0m \n",
            "GiB total capacity; \u001b[1;36m12.77\u001b[0m GiB already allocated; \u001b[1;36m8.81\u001b[0m MiB free; \u001b[1;36m13.69\u001b[0m GiB \n",
            "reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
            "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory \n",
            "Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "# 수정 학습 파라메터\n",
        "!python finetune.py \\\n",
        "    --base_model 'decapoda-research/llama-7b-hf' \\\n",
        "    --data_path 'self_instructs_output.json' \\\n",
        "    --output_dir './output' \\\n",
        "    --num_epochs 1 \\\n",
        "    --learning_rate 5e-4 \\\n",
        "    --val_set_size 10 \\\n",
        "    --batch_size 512 \\\n",
        "    --micro_batch_size 16 \\\n",
        "    --prompt_template_name 'custom'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35MJD3oeT0NP"
      },
      "outputs": [],
      "source": [
        "#!mkdir /content/drive/MyDrive/LLaMa-Alpaca-LoRA\n",
        "!cp -a output_third /content/drive/MyDrive/LLaMa-Alpaca-LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHu7NyrkPtua"
      },
      "source": [
        "## Test on Gradio\n",
        "\n",
        "--load_8bit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 권장 실행 명령어\n",
        "# !python generate.py \\\n",
        "#     --base_model 'decapoda-research/llama-7b-hf' \\\n",
        "#     --lora_weights '/content/drive/MyDrive/LLaMa-Alpaca-LoRA/output' \\\n",
        "#     --prompt_template 'custom' \\\n",
        "#     --share_gradio"
      ],
      "metadata": {
        "id": "eHhFleNqxkaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ygvnTjfjKWwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfafbe10-a698-4edb-8a1b-9faf5f653313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-22 13:14:01.756266: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-70jr6b3znmov --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "Loading checkpoint shards: 100% 33/33 [01:16<00:00,  2.33s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:30: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  super().__init__(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:30: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  super().__init__(\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://feaa466ac5041be031.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://feaa466ac5041be031.gradio.live\n"
          ]
        }
      ],
      "source": [
        "# 수정 실행 명령어\n",
        "!python generate.py \\\n",
        "    --base_model 'decapoda-research/llama-7b-hf' \\\n",
        "    --lora_weights '/content/drive/MyDrive/LLaMa-Alpaca-LoRA/output_third' \\\n",
        "    --prompt_template 'custom' \\\n",
        "    --share_gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c7rVMa2ll_G3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}